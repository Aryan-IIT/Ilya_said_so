# Ilya_said_so
Hacking around on Ilya Sutskever's top 30 AI papers. This repository collects canonical deep‑learning papers. For each, I’ll read, code from scratch (or adapt), and note my progress.

[List of Papers](https://aman.ai/primers/ai/top-30-papers/)


## Reading & Implementation List

| #  | Paper Title                                                                 | Key Idea / Summary                                                                    | Status       | Code link                      |
|----|------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|--------------|--------------------------------|
| 1  | The First Law of Complexodynamics                                            | “Complexodynamics”: study of complexity growth in closed systems                       |  code/complexodynamics.ipynb  | —                              |
| 2  | The Unreasonable Effectiveness of Recurrent Neural Networks                  | RNNs can learn complex sequences (text, code) almost “unreasonably” well               | Not started  | —                              |
| 3  | Understanding LSTM Networks                                                  | Intuitive, visual guide to LSTM internals and gating mechanisms                       | Not started  | —                              |
| 4  | Recurrent Neural Network Regularization                                      | Techniques (dropout variants) to reduce overfitting in RNNs                            | Not started  | —                              |
| 5  | Keeping Neural Networks Simple by Minimizing the Description Length of Weights | MDL principle to encourage simpler weight encodings                                   | Not started  | —                              |
| 6  | Pointer Networks                                                              | Sequence‑to‑sequence model that “points” to positions in the input                      | Not started  | —                              |
| 7  | ImageNet Classification with Deep Convolutional Neural Networks               | AlexNet: deep CNN breakthrough on ImageNet                                            | Not started  | —                              |
| 8  | Order Matters: Sequence to Sequence for Sets                                 | How to apply seq2seq to sets, handling permutation invariance                          | Not started  | —                              |
| 9  | GPipe: Easy Scaling with Micro‑Batch Pipeline Parallelism                      | Model‑parallelism via pipelining to train huge models                                 | Not started  | —                              |
| 10 | Deep Residual Learning for Image Recognition                                 | ResNet: residual connections enabling very deep CNNs                                  | Not started  | —                              |
| 11 | Multi‑Scale Context Aggregation by Dilated Convolutions                       | Dilated convolutions to aggregate multi‑scale context without losing resolution       | Not started  | —                              |
| 12 | Neural Message Passing for Quantum Chemistry                                 | Graph neural nets for predicting molecular properties                                 | Not started  | —                              |
| 13 | Attention is All You Need                                                     | Transformer architecture using self‑attention, no recurrence                          | Not started  | —                              |
| 14 | Neural Machine Translation by Jointly Learning to Align and Translate         | Bahdanau et al.: first attention in NMT                                              | Not started  | —                              |
| 15 | Identity Mappings in Deep Residual Networks                                   | Improved ResNet with pre‑activation and identity shortcuts                            | Not started  | —                              |
| 16 | A Simple Neural Network Module for Relational Reasoning                       | “Relation Network” module for reasoning about object pairs                            | Not started  | —                              |
| 17 | Variational Lossy Autoencoder                                                 | VAE variant that models lossy compression explicitly                                  | Not started  | —                              |
| 18 | Relational Recurrent Neural Networks                                          | RNNs augmented with relational reasoning modules                                      | Not started  | —                              |
| 19 | Quantifying the Rise and Fall of Complexity in Closed Systems: the Coffee Automaton | Empirical study of complexity evolution in a simple cellular automaton                 | Not started  | —                              |
| 20 | Neural Turing Machines                                                         | Neural nets with external memory and read/write heads                                 | Not started  | —                              |
| 21 | Deep Speech 2: End‑to‑End Speech Recognition in English and Mandarin           | End‑to‑end convolutional + recurrent model for speech recognition                     | Not started  | —                              |
| 22 | Scaling Laws for Neural Language Models                                       | Empirical power‑law relationships between model size, data, and performance            | Not started  | —                              |
| 23 | A Tutorial Introduction to the Minimum Description Length Principle           | MDL tutorial: formal link between compression and learning                           | Not started  | —                              |
| 24 | Machine Super Intelligence                                                     | Survey/discussion on pathways to superintelligent AI                                  | Not started  | —                              |
| 25 | Kolmogorov Complexity and Algorithmic Randomness                              | Foundations of algorithmic information theory                                         | Not started  | —                              |
| 26 | Stanford’s CS231n Convolutional Neural Networks for Visual Recognition        | Lecture notes and assignments covering CNN fundamentals and advanced topics           | Not started  | —                              |



