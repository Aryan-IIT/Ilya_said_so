{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964c599b",
   "metadata": {},
   "source": [
    "[Link to paper](https://scottaaronson.blog/?p=762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812298d4",
   "metadata": {},
   "source": [
    "![](image.png)\n",
    "\n",
    "\n",
    "**Question**: why does “complexity” or “interestingness” of physical systems seem to increase with time and then hit a maximum and decrease, in contrast to the entropy, which of course increases monotonically?\n",
    "\n",
    "### Challenge?\n",
    "1. Come up with a plausible formal definition of “complexity.”\n",
    "2. Prove that the “complexity,” so defined, is large at intermediate times in natural model systems, despite being close to zero at the initial time and close to zero at late times.\n",
    "\n",
    "### Sophisctication\n",
    "\n",
    "Kolmogorov complexity:  \n",
    "$K(s)$ of a string $s$ is the length (in bits) of the shortest program that prints out exactly $s$.\n",
    "\n",
    "Blah blah blah\n",
    "\n",
    "## Takeaway for AI folks \n",
    "\n",
    "Thought for a second\n",
    "\n",
    "\n",
    "1. **Choosing the right model (“bias–capacity tradeoff”)**\n",
    "\n",
    "   * Think of your neural network as a limited‑size suitcase: you can only pack so many patterns.\n",
    "   * Resource‑bounded complexity asks, “Which patterns give you the biggest payoff for the bits you spend?”\n",
    "   * In practice, that guides you to pick a model size that captures real structure in the data without memorizing noise.\n",
    "\n",
    "2. **How learning rises and then overfits (“double descent”)**\n",
    "\n",
    "   * Early in training, the network is too simple—underfits.  Later, it’s just right—learns useful features.  If you push it longer or bigger, it can start memorizing—overfits.\n",
    "   * That “rise → peak → fall” in test performance matches the complextropy curve: the amount of meaningful structure the model encodes.\n",
    "\n",
    "3. **A new way to watch training (“tracking complextropy”)**\n",
    "\n",
    "   * Instead of only plotting loss or accuracy, imagine computing a quick “compressibility” score on the hidden‑layer activations.\n",
    "   * When that score peaks, your network’s internal representations are richest.  That tells you the best stopping point or where to tweak capacity.\n",
    "\n",
    "---\n",
    "\n",
    "**In one sentence:**\n",
    "By measuring how much a **bounded** learner really needs to describe its data (complextropy), you get a simple guide for picking model size, spotting when it’s learned the most useful features, and knowing exactly when to stop training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e167fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
